{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem description.\n",
      "Vipul is a hardworking super-hero who maintains the bracket ratio of all the strings in the world. Recently he indulged himself in saving the string population so much that he lost his ability for checking brackets (luckily, not permanently ).Being his super-hero friend help him in his time of hardship. \n",
      "\n",
      "Input\n",
      "\n",
      "The first line of the input contains an integer T denoting the number of test cases. The description of T test cases follows.\n",
      "The first line of each test case contains a single string S denoting the string to be checked.\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "For each test case, output a single line printing \"YES\" or \"NO\" (without \" \" and in uppercase only) , denoting if the brackets in the given string is balanced or not .\n",
      "\n",
      "\n",
      "Constraints\n",
      "\n",
      "1 ≤ T ≤ 10\n",
      "1 ≤ length of S ≤ 60\n",
      "\n",
      "\n",
      "Example\n",
      "Input:\n",
      "3\n",
      "((()))\n",
      "(())()\n",
      "()(()\n",
      "\n",
      "Output:\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      " \n",
      "\n",
      "Explanation\n",
      "Example is self-explanatory.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True) #cf_tags\n",
    "for it in dataset:\n",
    "    print(it['description'])\n",
    "    print(it['cf_tags'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(split):\n",
    "    dataset = load_dataset(\"deepmind/code_contest\", split=split)   \n",
    "    tags = []\n",
    "    descriptions = [] \n",
    "    for x in dataset:\n",
    "        if(len(dataset['cf_tags'] == 0)) continue\n",
    "        tags.append(dataset['cf_tags'])\n",
    "        descriptions.append(dataset['description'])\n",
    "    return tags, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetFromTags(tag_list):\n",
    "    my_set = set()\n",
    "    for sub_list in my_list:\n",
    "        for element in sub_list:\n",
    "            my_set.add(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(tag_list, tag_set):\n",
    "    index_dict = {}\n",
    "    for i, element in enumerate(tag_set):\n",
    "        index_dict[element] = i\n",
    "\n",
    "    one_hot_list = torch.zeros((len(tag_list),len(tag_set)))\n",
    "    for i, sub_list in enumerate(tag_list):\n",
    "        for element in sub_list:\n",
    "            idx = index_dict[element]\n",
    "            one_hot_list[i][idx]=1\n",
    "    \n",
    "    return one_hot_list, index_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x, tr_tags = getDataset('train')\n",
    "tags_set = getSetFromTags(tr_tags)\n",
    "tr_y, tags_dict = getOneHot(tr_tags, tags_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagsDataset(Dataset):\n",
    "    def __init__(self, descriptions, labels, one_hot):\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.Y = one_hot\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = (torch.Tensor(self.data[index]), torch.Tensor(self.labels[index]))\n",
    "        y = torch.Tensor(self.Y[index])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TagsDataset(tr_x, tr_tags, tr_y)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEAM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(LEAM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = nn.Linear(embedding_dim, num_classes, bias=False)\n",
    "        self.label_embeddings = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # x: input text sequence, shape (batch_size, seq_length)\n",
    "        # labels: multi-hot encoded label vectors, shape (batch_size, num_classes)\n",
    "\n",
    "        # Embed the input text sequence\n",
    "        x_embedded = self.embedding(x)  # shape (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        # Compute attention weights for each class\n",
    "        attention_weights = self.attention(x_embedded)  # shape (batch_size, seq_length, num_classes)\n",
    "\n",
    "        # Apply softmax activation to get attention probabilities\n",
    "        attention_probs = F.softmax(attention_weights, dim=1)  # shape (batch_size, seq_length, num_classes)\n",
    "\n",
    "        # Weight the label embeddings by the attention probabilities\n",
    "        label_weights = torch.bmm(attention_probs.transpose(1, 2), self.label_embeddings(labels))  # shape (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute the final prediction scores\n",
    "        prediction = torch.sum(x_embedded * attention_probs, dim=1) + label_weights  # shape (batch_size, embedding_dim)\n",
    "        prediction = F.sigmoid(prediction)  # shape (batch_size, num_classes)\n",
    "\n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
